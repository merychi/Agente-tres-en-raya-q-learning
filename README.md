# ğŸ§  Tres en Raya (Tic-Tac-Toe) con Agente Machine Learning

![Banner](https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python&logoColor=white)
![Banner](https://img.shields.io/badge/Pygame-CE-purple?style=for-the-badge&logo=sdl&logoColor=white)
![Banner](https://img.shields.io/badge/AI-Minimax-red?style=for-the-badge)

> **TRES EN RAYAS:** EnfrÃ©ntate a una Inteligencia Artificial de Machine Learning y observa cÃ³mo compite contra el agente Minimax

---

## ğŸ“¸ GalerÃ­a

| MenÃº Principal | Juego Modo Humano Vs Agente |
|:---:|:---:|
| ![MenÃº](assets/screenshots/menu.png) | ![Juego Modo Humano Vs Agente](assets/screenshots/jugador_vs_ai.png) |

| Juego Modo Agente QL Vs Agente Minimax | Modal del Ã¡rbol de decisiones |
|:---:|:---:|
| ![Juego Modo Agente QL Vs Agente Minimax](assets/screenshots/minimax_vs_ql.png) | ![Modal del Ã¡rbol de decisiones](assets/screenshots/modal_arbol.png) |

---

## âœ¨ CaracterÃ­sticas Principales

### ğŸ¤– Agente de Inteligencia Artificial
*   ImplementaciÃ³n del algoritmo **Q-Learning** entrenada mediante aprendizaje por refuerzo.
*   La IA juega de forma Ã³ptima: siempre buscarÃ¡ ganar o, en el peor de los casos, forzar un empate. Â¡Intenta ganarle si puedes!

### ğŸ¨ Interfaz GrÃ¡fica 
*   DiseÃ±o estilo *cartoon* con colores modernos (paleta morada/azul).
*   **Botones interactivos:** Efectos de elevaciÃ³n, sombras y sonidos al pasar el mouse.
*   **Feedback visual:** Animaciones al colocar fichas y lÃ­nea dorada al ganar.
*   **Avatar:** Los agentes de inteligencia artificial se muestran en pantalla como gatos.

### ğŸ± Modo de Juego
*   **Humano Vs Agente ML:** Juega contra un agente de inteligencia artificial entrenado por aprendizaje por refuerzo.
*   **Agente ML vs Agente Minimax:** Observa cÃ³mo juegan dos agentes de inteligencia artificial entrenados para jugar de la manera mÃ¡s Ã³ptima posible; observa al final el Ã¡rbol de decisiones de cada una de sus jugadas.

---

## ğŸš€ InstalaciÃ³n y EjecuciÃ³n

Sigue estos pasos para probar el proyecto en tu mÃ¡quina local:

1.  **Clonar el repositorio:**
    ``` bash
    git clone https://github.com/Jumicode/Agente-tres-en-raya-q-learning
    cd agente-tres-rayas-q-learning
    ```

2.  **Crear un entorno virtual (Opcional pero recomendado):**
    ``` bash
    python -m venv venv
    source venv/bin/activate  # En Windows: venv\Scripts\activate
    ```

3.  **Instalar dependencias:**
    Solo necesitas `pygame`.
    ``` bash
    pip install pygame
    ```

4.  **Â¡Jugar!**
    ```
    python main.py
    ```    
---

## ğŸ“Š Â¿CÃ³mo puedes entrenar al Agente?

1.  **Elimina el archivo "conocimiento_gato.json**

2.  **Ejecuta el comando para entrenar al agente desde la raÃ­z del proyecto**
    ``` bash
    python -m game.trainer
    cd agente-tres-rayas-q-learning
    ```
3.  **En consola podrÃ¡s observar el tiempo y cÃ³mo se entrenÃ³ el agente**

4.  **PodrÃ¡s generar un reporte en html del entrenamiento del agente ejecutando el comando:**
    ``` bash
    python generar_reporte.py
    cd agente-tres-rayas-q-learning
    ```
5.  **Abre el archivo "REPORTE_CEREBRO_AI.html en tu navegador**

---

## ğŸ“‚ Estructura del Proyecto

El cÃ³digo estÃ¡ modularizado para mantener el orden y la escalabilidad:

```text
agente-tres-rayas-q-learning/
â”œâ”€â”€ ğŸ“‚ assets/          # ImÃ¡genes, fuentes (.ttf) y sonidos (.wav/.mp3)
â”œâ”€â”€ ğŸ“‚ game/            # LÃ³gica del juego
â”‚   â”œâ”€â”€ ai.py           # Algoritmo q-learning, minimax y generaciÃ³n de Ã¡rboles
â”‚   â”œâ”€â”€ logic.py        # Reglas del Tres en Raya
â”‚   â””â”€â”€ trainer.py      # MÃ³dulo para entrenar al Agente Q-Learning
â”œâ”€â”€ ğŸ“‚ ui/              # Interfaz de Usuario
â”‚   â”œâ”€â”€ assets.py       # Funciones para cargar recursos (Fuentes, Sonidos, etc.)
â”‚   â”œâ”€â”€ config.py       # Colores y constantes
â”‚   â”œâ”€â”€ components.py   # Elementos reutilizables (mini tableros, botones)
â”‚   â”œâ”€â”€ interface.py    # Pantalla principal del juego
â”‚   â”œâ”€â”€ menu.py         # MenÃº principal animado
â”‚   â”œâ”€â”€ help.py         # Pantalla de "CÃ³mo jugar"
â”‚   â””â”€â”€ events.py       # Manejo de inputs del usuario
â”œâ”€â”€ conocimiento_gato.json # Archivo de memoria del Agente Q-Learning
â”œâ”€â”€ generar_reporte.py  # Genera el HTML con el resultado del entrenamiento
â””â”€â”€ main.py             # Punto de entrada y bucle del juego           
```

## ğŸ§  Â¿CÃ³mo funciona el algoritmo?

A diferencia de los algoritmos de bÃºsqueda tradicionales como Minimax, este proyecto implementa un agente de Aprendizaje por Refuerzo (Reinforcement Learning) que no conoce las reglas del juego de antemano, sino que las descubre a travÃ©s de la experiencia.

1. El agente no genera un Ã¡rbol de bÃºsqueda exhaustivo. En su lugar, utiliza una Tabla Q (Q-Table), que actÃºa como una "memoria" donde almacena quÃ© tan buena es cada acciÃ³n para cada estado posible del tablero.
2. Utiliza la EcuaciÃ³n de Bellman como motor de aprendizaje. Cada vez que el agente realiza un movimiento y recibe una recompensa, utiliza esta fÃ³rmula matemÃ¡tica para actualizar el valor de esa jugada en su memoria, considerando tanto el beneficio inmediato como la promesa de ganar en el futuro.
3. Posee un Sistema de Recompensas, donde el agente es entrenado mediante un sistema de estÃ­mulos:
   * +10 puntos por ganar (fomenta la ofensiva).
   * +5 puntos por empatar (fomenta la defensa sÃ³lida).
   * -10 puntos por perder (penaliza errores crÃ­ticos).
4. Dilema ExploraciÃ³n vs. ExplotaciÃ³n (Epsilon-Greedy):
   * ExploraciÃ³n: Al inicio del entrenamiento, el agente realiza movimientos al azar para descubrir nuevas jugadas.
   * ExplotaciÃ³n: Conforme acumula experiencia, el agente reduce su curiosidad y comienza a "explotar" su memoria, eligiendo siempre la jugada con el Valor Q mÃ¡s alto.
5. El agente se ve sometido a un Entrenamiento Masivo de 10,000 episodios (partidas), permitiÃ©ndole alcanzar un estado de racionalidad perfecta donde es capaz de empatar siempre contra un algoritmo Minimax y derrotar sistemÃ¡ticamente a jugadores humanos.


## ğŸ‘¥ CrÃ©ditos
Desarrollado por:
- [Merry-am Blanco](https://github.com/merychi)
- [Julio Romero](https://github.com/Jumicode)